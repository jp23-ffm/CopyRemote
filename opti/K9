def consolidate_k9_streaming(csv_file_path):
    """
    Streaming consolidation with memory cleanup
    """
    
    write_log(f"[{datetime.now()}] Starting streaming consolidation...")
    
    # Dictionary to store groups
    groups = defaultdict(lambda: {'non_k9': {}, 'k9_values': defaultdict(set)})
    
    k9_prefixes = ['K9-APPLICATIONS']
    
    # ===== PHASE 1: READ AND GROUP =====
    with open(csv_file_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        fieldnames = reader.fieldnames
        
        k9_cols = [col for col in fieldnames if any(prefix in col for prefix in k9_prefixes)]
        non_k9_cols = [col for col in fieldnames if col not in k9_cols]
        
        write_log(f"[{datetime.now()}] Found {len(k9_cols)} K9 columns and {len(non_k9_cols)} non-K9 columns")
        
        row_count = 0
        
        for row in reader:
            row_count += 1
            
            if row_count % 10000 == 0:
                write_log(f"[{datetime.now()}] Processed {row_count} rows, {len(groups)} unique groups")
            
            group_key_parts = [row.get(col, '') for col in non_k9_cols]
            group_key = '|||'.join(group_key_parts)
            
            if not groups[group_key]['non_k9']:
                groups[group_key]['non_k9'] = {col: row.get(col, '') for col in non_k9_cols}
            
            for k9_col in k9_cols:
                value = row.get(k9_col, '').strip()
                if value and value.upper() not in ['N/A', 'NAN', 'NONE', '']:
                    groups[group_key]['k9_values'][k9_col].add(value)
    
    write_log(f"[{datetime.now()}] Grouping complete: {row_count} rows â†’ {len(groups)} groups")
    
    # ===== PHASE 2: WRITE AND CLEANUP =====
    output_path = csv_file_path.replace('.csv', '_consolidated.csv')
    
    write_log(f"[{datetime.now()}] Writing consolidated data to {output_path}...")
    
    with open(output_path, 'w', encoding='utf-8', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        
        written_count = 0
        
        for group_data in groups.values():
            row_out = group_data['non_k9'].copy()
            
            for k9_col in k9_cols:
                values = group_data['k9_values'].get(k9_col, set())
                if values:
                    row_out[k9_col] = ' | '.join(sorted(values))
                else:
                    row_out[k9_col] = ''
            
            writer.writerow(row_out)
            written_count += 1
            
            if written_count % 10000 == 0:
                write_log(f"[{datetime.now()}] Written {written_count} rows")
    
    consolidated_count = len(groups)
    
    write_log(f"[{datetime.now()}] File written successfully")
    
    # ===== PHASE 3: FREE MEMORY =====
    write_log(f"[{datetime.now()}] Freeing memory...")
    
    # Clear the dictionary
    groups.clear()
    
    # Force garbage collection to reclaim memory immediately
    import gc
    gc.collect()
    
    write_log(f"[{datetime.now()}] Memory freed")
    write_log(f"[{datetime.now()}] Original rows: {row_count}")
    write_log(f"[{datetime.now()}] Consolidated rows: {consolidated_count}")
    
    return output_path, row_count, consolidated_count
